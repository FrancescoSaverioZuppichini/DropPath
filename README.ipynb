{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f89cf6",
   "metadata": {},
   "source": [
    "# Implementing Stochastic Depth/Drop Path In PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3061e6e4",
   "metadata": {},
   "source": [
    "DropPath is available on [glasses](https://github.com/FrancescoSaverioZuppichini/glasses) my computer vision library!\n",
    "\n",
    "Code is [here](https://github.com/FrancescoSaverioZuppichini/DropPath), an interactive version of this article can be downloaded from [here](https://github.com/FrancescoSaverioZuppichini/DropPath/blob/main/README.ipynb).\n",
    "\n",
    "## Introduction\n",
    "Today we are going to implement Stochastic Depth also known as Drop Path in PyTorch! [Stochastic Depth](https://arxiv.org/abs/1603.09382) introduced by Gao Huang et al is technique to \"deactivate\" some layers during training.  \n",
    "\n",
    "Let's take a look at a normal ResNet Block that uses residual connections (like almost all models now).If you are not familiar with ResNet, I have an [article](https://towardsdatascience.com/residual-network-implementing-resnet-a7da63c7b278) showing how to implement it. \n",
    "\n",
    "Basically, the block's output is added to its input: `output = block(input) + input`. This is called a **residual connection**\n",
    "\n",
    "![alt](./images/ResNetBlock.svg)\n",
    "\n",
    "\n",
    "Here we see four ResnNet like  blocks, one after the other. \n",
    "\n",
    "![alt](./images/StochasticDepthBase.svg)\n",
    "\n",
    "Stochastic Depth/Drop Path will deactivate some of the block's weight\n",
    "\n",
    "![alt](./images/StochasticDepthActive.svg)\n",
    "\n",
    "The idea is to reduce the number of layers/block used during training, saving time and make the network generalize better. \n",
    "\n",
    "Practically, this means setting to zero the output of the block before adding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce71cc8",
   "metadata": {},
   "source": [
    "## Implementation \n",
    "Let's start by importing our best friend, `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda6e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d58d4",
   "metadata": {},
   "source": [
    "We can define a 4D tensor (`batch x channels x height x width`), in our case let's just send 4 images with one pixel each, so it's easier to see what's going on :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0366dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((4, 1, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44cecc5",
   "metadata": {},
   "source": [
    "We need a tensor of shape `batch x 1 x 1 x 1` that will be used to set some of the elements in the batch to zero, using a given prob. Bernoulli to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67000ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_prob: float = .5\n",
    "mask: Tensor = x.new_empty(x.shape[0], 1, 1, 1).bernoulli_(keep_prob)\n",
    "    \n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7624b3",
   "metadata": {},
   "source": [
    "Btw, this is equivelant to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d0d6744",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask: Tensor = (torch.rand(x.shape[0], 1, 1, 1) > keep_prob).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658e3ef",
   "metadata": {},
   "source": [
    "We want to set some of `x`'s elements to zero, since our masks is composed by `0`s and `1`s, we can multiply it by `x`. Before we do it, we need to divide `x` by  `keep_prob` to rescale down the inputs activation during training, see [cs231n](https://cs231n.github.io/neural-networks-2/#reg). So"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c6eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled : Tensor = x / keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4e92b",
   "metadata": {},
   "source": [
    "Finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67692f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2.]]],\n",
       "\n",
       "\n",
       "        [[[2.]]],\n",
       "\n",
       "\n",
       "        [[[2.]]],\n",
       "\n",
       "\n",
       "        [[[2.]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output: Tensor = x_scaled * mask\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b1939",
   "metadata": {},
   "source": [
    "We can put it together in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d4d65bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]]],\n",
       "\n",
       "\n",
       "        [[[2.]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_path(x: Tensor, keep_prob: float = 1.0) -> Tensor:\n",
    "    mask: Tensor = x.new_empty(x.shape[0], 1, 1, 1).bernoulli_(keep_prob)\n",
    "    x_scaled: Tensor = x / keep_prob\n",
    "    return x_scaled * mask\n",
    "\n",
    "drop_path(x, keep_prob=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89952f5b",
   "metadata": {},
   "source": [
    "We can also do the operations inplace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97cc21e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2.]]],\n",
       "\n",
       "\n",
       "        [[[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]]],\n",
       "\n",
       "\n",
       "        [[[2.]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_path(x: Tensor, keep_prob: float = 1.0) -> Tensor:\n",
    "    mask: Tensor = x.new_empty(x.shape[0], 1, 1, 1).bernoulli_(keep_prob)\n",
    "    x.div_(keep_prob)\n",
    "    x.mul_(mask)\n",
    "    return x\n",
    "\n",
    "\n",
    "drop_path(x, keep_prob=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858ed8c",
   "metadata": {},
   "source": [
    "However, we may want to use `x` somewhere else, and dividing `x` or `mask` by `keep_prob` is the same thing. Let's arrive at the final implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7156fbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]]],\n",
       "\n",
       "\n",
       "        [[[0.]]],\n",
       "\n",
       "\n",
       "        [[[2.]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_path(x: Tensor, keep_prob: float = 1.0, inplace: bool = False) -> Tensor:\n",
    "    mask: Tensor = x.new_empty(x.shape[0], 1, 1, 1).bernoulli_(keep_prob)\n",
    "    mask.div_(keep_prob)\n",
    "    if inplace:\n",
    "        x.mul_(mask)\n",
    "    else:\n",
    "        x = x * mask\n",
    "    return x\n",
    "\n",
    "x = torch.ones((4, 1, 1, 1))\n",
    "drop_path(x, keep_prob=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad02160",
   "metadata": {},
   "source": [
    "`drop_path` only works for 2d data, we need to automatically calculate the number of dimensions from the input size to make it work for any data time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6974fc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_path(x: Tensor, keep_prob: float = 1.0, inplace: bool = False) -> Tensor:\n",
    "    mask_shape: Tuple[int] = (x.shape[0],) + (1,) * (x.ndim - 1) \n",
    "    # remember tuples have the * operator -> (1,) * 3 = (1,1,1)\n",
    "    mask: Tensor = x.new_empty(mask_shape).bernoulli_(keep_prob)\n",
    "    mask.div_(keep_prob)\n",
    "    if inplace:\n",
    "        x.mul_(mask)\n",
    "    else:\n",
    "        x = x * mask\n",
    "    return x\n",
    "\n",
    "x = torch.ones((4, 1))\n",
    "drop_path(x, keep_prob=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a45f899",
   "metadata": {},
   "source": [
    "Let's create a nice `DropPath` `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be9c0f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self, p: float = 0.5, inplace: bool = False):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.training and self.p > 0:\n",
    "            x = drop_path(x, self.p, self.inplace)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(p={self.p})\"\n",
    "\n",
    "    \n",
    "DropPath()(torch.ones((4, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be1582",
   "metadata": {},
   "source": [
    "## Usage with Residual Connections\n",
    "\n",
    "We have our `DropPath`, cool but how do we use it? We need a residual block, let's use a classic ResNet block: the good old friend `BottleNeckBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "885acdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class ConvBnAct(nn.Sequential):\n",
    "    def __init__(self, in_features: int, out_features: int, kernel_size=1):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_features, out_features, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.BatchNorm2d(out_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "         \n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, reduction: int = 4):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            # wide -> narrow\n",
    "            ConvBnAct(in_features, out_features // reduction, kernel_size=1),\n",
    "            # narrow -> narrow\n",
    "            ConvBnAct( out_features // reduction, out_features // reduction, kernel_size=3),\n",
    "            # wide -> narrow\n",
    "            ConvBnAct( out_features // reduction, out_features, kernel_size=1),\n",
    "        )\n",
    "        # I am lazy, no shortcut etc\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        return x + res\n",
    "    \n",
    "    \n",
    "BottleNeck(64, 64)(torch.ones((1,64, 28, 28))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242dbfb3",
   "metadata": {},
   "source": [
    "To deactivate the block the operation `x + res` must be equal to `res`, so our `DropPath` has to be applied after the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4778571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, reduction: int = 4):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            # wide -> narrow\n",
    "            ConvBnAct(in_features, out_features // reduction, kernel_size=1),\n",
    "            # narrow -> narrow\n",
    "            ConvBnAct( out_features // reduction, out_features // reduction, kernel_size=3),\n",
    "            # wide -> narrow\n",
    "            ConvBnAct( out_features // reduction, out_features, kernel_size=1),\n",
    "        )\n",
    "        # I am lazy, no shortcut etc\n",
    "        self.drop_path = DropPath()\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        res = x\n",
    "        x = self.block(x)\n",
    "        x = self.drop_path(x)\n",
    "        return x + res\n",
    "    \n",
    "BottleNeck(64, 64)(torch.ones((1,64, 28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631cf44",
   "metadata": {},
   "source": [
    "Tada🎉! Now, randomly, our `.block` will be completely skipped! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d3a3f",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this article, we have seen how to implement DropPath and use it inside a residual block. Hopefully, when you'll read/see drop path/stochastic depth you know how it's made \n",
    "\n",
    "Take care :)\n",
    "\n",
    "Francesco"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
